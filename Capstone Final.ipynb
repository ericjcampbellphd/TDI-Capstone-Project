{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#            Stock Market Quarterly Investment Recommender\n",
    "\n",
    "A Data Science Project by Eric J Campbell for The Data Incubator 2019 NYC Fall Cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model aims to help an investor decide what stocks to invest in or what stocks to sell by attempting to predict the outcome of a publicly-traded company's quarterly earnings report. One of the key metrics released during each quarter is the Earnings Per Share (EPS), which is a reflection of the companies performance. If the EPS exceeds expectations, investors usually invest more in the company, subsequently raising the share price. Conversely, if the EPS falls below expectations, investors are prone to selling the stock, which causes a fall in share price. This model uses a freely-avaibable Facebook dataset which tracks the checkins, likes, and talking about counts for many companies over time. The checkins are used as a feature to quantify the trends in consumer activity at a physical company storefront, and are therefore a reflection of company performance. To summarize, by using data leading up to the release of a quarterly earnings report, this model will recommend to buy or sell a stock depending on predicted performance, which can both maximize profit and minimize loss while investing.\n",
    "\n",
    "Data are found from multiple sources. The Facebook dataset, which is around 0.5 GB in size, is provided freely by Thinknum. Furthermore, two financial datasets are found using webscraping and availible APIs. Reported and expected EPS data, and release dates are found by web-scraping Yahoo Finance using the python Requests library and Beautiful Soup. Stock price, as well as quarterly revenue, profit, loss, and quarter dates are retrieved using a financial API provided by Intrinio. The Facebook dataset contains both public and private companies, and does not include stock information. The Intrinio API is again used to search for similar sounding companies, which are then processed using the Fuzzy Wuzzy library to find the best company match and stock ticker.\n",
    "\n",
    "A machine learning model was implemented in order to predict the surprise EPS. The input dataframe was constructed as follows. Each row serves as one observation of a company's quarterly results. Six features are used, including the average checkins, average likes, and average talking about counts for that quarter, as well as the reported EPS. The remaining two features are found from feature engineering, using the ratio of both total gross profit and total operating expenses to the total revenue for that quarter. Because the likes, checkins, and talking about counts vary over several orders of magnitude across different companies, a StandardScaler transformer was constructed to scale the input data to more reasonable magnitudes. This transformer is placed in a pipeline along with a random forest predictor, and a gridsearch is performed to find optimal parameters. The accuracy is found to be ~0.95 for the training set and ~0.40 for the test set. It should be noted that the low performance seen in the test set is likely due to the quality of the data which includes companies with no physical presence or publicly-traded stock."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A validation of the idea behind this project is presented in the chart below, which shows the stock price for Tesla over the last two years, along with the points where a quarterly report was issued. Both positive EPS results (green dots) and negative EPS results (red dots) can be seen. Most of the time, the EPS report generates a predictable outcome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tesla_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"tesla_plot.png\" style=\"width: 900px, height: 500px;\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final product of the model is shown below. \n",
    "\n",
    "By selecting a company, year, and quarter, the model will build a sample portfolio of what stocks should be owned, and what stocks should not be owned. Each row is color-coded to that effect:\n",
    "1. Dark Green - Strong Buy: The stock is predicted to spike in price with the EPS report release.\n",
    "2. Light Green - Weak Buy: This stock is predicted to increase slightly with the EPS report release.\n",
    "3. Pink - Weak Sell: This stock is predicted to decrease slightly with the EPS report release.\n",
    "4. Red - Strong Sell: This stock is expected to drop with the EPS report release.\n",
    "\n",
    "The upcoming report release date, predicted EPS, and predictor recommendation are also provided. It is noted here that only a select few companies have been included in the result, as they are publicly-traded brick-and mortar stores where checkin activity is relevant. With access to more data on physical companies, this predictor would be more diversified and therefore, safer for an investor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"table.jpg\" style=\"width: 200px, height: 200px;\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, a sample output of the interactive portfolio is shown for the first quarter of 2017. The predictor recommends buying shares in each company with a strong certainty for the dark green rows, and with some certainty for the light green rows. Report dates are also provided so an investor knows when the buy and sell should take place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, an interactive plot is presented below, where given the year and quarter, the portfolio yield from the ML predictor model is plotted. This portfolio is then compared to a naive model which uses a Dow Jones tracked index fund ETF. The resulting portfolio return for both models is then displayed for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"plot1.jpg\" style=\"width: 400px, height: 400px;\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows the growth of a portfolio for the machine learning model as well as a naive model. For this quarter, the return on investment is comparable between both models, though the return from the ML model is achieved faster than in the naive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"plot2.jpg\" style=\"width: 400px, height: 400px;\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another case of the interactive plotting tool is shown above for the year 2018. In this case, although money is still lost, the machine learning model outperforms the naive model. Further examples can be generated showing the ML model's performance against the naive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaway and Project Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report, a machine learning model was conceived and built from start to finish using freely availible data, and data from APIs and webscraping. Non-trivial analysis was performed on the data and fed into the model to generate complex predictions. Finally, an interactive portfolio generator and investment yield estimator were constructed in order to provide an investor a simple guide towards investing.\n",
    "\n",
    "By using this model, an investor could potentially earn large investment yields in short periods of time by taking advantage of the volatility in a stock's share price during a quarterly earnings report release. Though an earnings report is not the sole variable in determining trends in a stock's price, it is by no means unimportant, as earnings are a very powerful metric for determining a company's performance. \n",
    "\n",
    "To conclude, although the model's predictive performance on the test set was ~0.4, it could be improved with further refinement and more amounts of high-quality data. To that end, this model could serve as a test case for an unsupervised real-time model which generates predictions for investors before a company's report release date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open('EPS_dates_p_tesla.pickle', 'rb') as handle:\n",
    "    EPS_dates_p = pickle.load(handle)\n",
    "with open('EPS_dates_n_tesla.pickle', 'rb') as handle:\n",
    "    EPS_dates_n = pickle.load(handle)\n",
    "with open('EPS_prices_p_tesla.pickle', 'rb') as handle:\n",
    "    EPS_prices_p = pickle.load(handle)\n",
    "with open('EPS_prices_n_tesla.pickle', 'rb') as handle:\n",
    "    EPS_prices_n = pickle.load(handle)\n",
    "with open('dates_tesla.pickle', 'rb') as handle:\n",
    "    dates = pickle.load(handle)\n",
    "with open('prices_tesla.pickle', 'rb') as handle:\n",
    "    prices = pickle.load(handle)\n",
    "\n",
    "X_test_predicted = pd.read_pickle('X_test_predicted.pkl')\n",
    "with open('quarter_release_date.pickle', 'rb') as handle:\n",
    "    quarter_release_date = pickle.load(handle)\n",
    "\n",
    "with open('dowjones_date_quarter_price_dict.pickle', 'rb') as handle:\n",
    "    dowjones_date_quarter_price_dict = pickle.load(handle)\n",
    "with open('select_profit.pickle', 'rb') as handle:\n",
    "    select_profit = pickle.load(handle)\n",
    "with open('select_cost.pickle', 'rb') as handle:\n",
    "    select_cost = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tesla_chart():\n",
    "    from datetime import datetime\n",
    "    from bokeh.plotting import figure, output_notebook, show, ColumnDataSource\n",
    "    from bokeh.transform import factor_cmap\n",
    "    from bokeh.models import HoverTool\n",
    "    import pickle\n",
    "    \n",
    "#     with open('EPS_dates_p_tesla.pickle', 'rb') as handle:\n",
    "#         EPS_dates_p = pickle.load(handle)\n",
    "#     with open('EPS_dates_n_tesla.pickle', 'rb') as handle:\n",
    "#         EPS_dates_n = pickle.load(handle)\n",
    "#     with open('EPS_prices_p_tesla.pickle', 'rb') as handle:\n",
    "#         EPS_prices_p = pickle.load(handle)\n",
    "#     with open('EPS_prices_n_tesla.pickle', 'rb') as handle:\n",
    "#         EPS_prices_n = pickle.load(handle)\n",
    "#     with open('dates_tesla.pickle', 'rb') as handle:\n",
    "#         dates = pickle.load(handle)\n",
    "#     with open('prices_tesla.pickle', 'rb') as handle:\n",
    "#         prices = pickle.load(handle)\n",
    "\n",
    "    output_notebook()\n",
    "\n",
    "    p = figure(\n",
    "       tools=['pan','box_zoom','reset','save'],\n",
    "       x_range=[datetime(2018, 1, 1).date(), datetime(2019, 11, 30).date()],\n",
    "        title=\"Tesla Stock Price with Quarterly Earnings Per Share Releases\",\n",
    "       x_axis_label='Date', y_axis_label='Stock Price',\n",
    "        x_axis_type=\"datetime\" ,\n",
    "    )\n",
    "    p.width = 900\n",
    "    p.height = 500\n",
    "    \n",
    "    p.line(dates, prices, legend='Price')\n",
    "    p.scatter(EPS_dates_p, EPS_prices_p, size=8, color='lime', legend='Positive Report')\n",
    "    p.scatter(EPS_dates_n, EPS_prices_n, size=8, color='red', legend='Negative Report')\n",
    "    p.legend.location = 'top_right'\n",
    "\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_table():\n",
    "    from IPython.display import display\n",
    "    from ipywidgets import widgets\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    \n",
    "#     X_test_predicted = pd.read_pickle('X_test_predicted.pkl')\n",
    "#     with open('quarter_release_date.pickle', 'rb') as handle:\n",
    "#         quarter_release_date = pickle.load(handle)\n",
    "    \n",
    "    company_list = ['WALMART',\n",
    "                        'TEXASROADHOUSE',\n",
    "                        'DENNYS',\n",
    "                        'DILLARDS',\n",
    "                        'BIGLOTS',\n",
    "                        'PLANETFITNESS',\n",
    "                        'BIG5SPORTINGGOODS',\n",
    "                        'LUMBERLIQUIDATORS',\n",
    "                        'CHIPOTLE',\n",
    "                        'DOLLARGENERAL',\n",
    "                        'REDROBIN',\n",
    "                        'DELTA',\n",
    "                        'DESTINATIONXL',\n",
    "                        'WINGSTOP',\n",
    "                        'SEAWORLD',\n",
    "                        'MCDONALDSUS',\n",
    "                        'NORWEGIANCRUISELINE',\n",
    "                        'ADVANCEAUTOPARTS',\n",
    "                        'CHILDRENSPLACE',\n",
    "                        'GUESS',\n",
    "                        'KROGER',\n",
    "                        'NORDSTROMRACK',\n",
    "                        'CAESARSENTERTAINMENTCORP',\n",
    "                        'ROYALCARIBBEAN',\n",
    "                        'FOOTLOCKER',\n",
    "                        'ESTEELAUDERCOMPANIES',\n",
    "                        'CRACKERBARREL',\n",
    "                        'AMERICANAIRLINES',\n",
    "                        'LAZBOY',\n",
    "                        'NIKE',\n",
    "                        'MARRIOTTINTERNATIONAL',\n",
    "                        'AUTONATION',\n",
    "                        'EXTENDEDSTAYAMERICA',\n",
    "                        'NATURALGROCERS',\n",
    "                        'SHAKESHACK',\n",
    "                        'POTBELLYSANDWICHSHOP',\n",
    "                        'KOHLS']\n",
    "\n",
    "    quarter_date_list = ['ALL QUARTERS', 'Jan 1 - Mar 31', 'Apr 1 - June 30', 'Jul 1 - Sep 30', 'Oct 1 - Dec 31']\n",
    "\n",
    "    def highlight_greaterthan_1(s):\n",
    "        if s['Predicted Surprise EPS'] >= 10.:\n",
    "            return ['background-color: green']*4\n",
    "        elif s['Predicted Surprise EPS'] > 0. and s['Predicted Surprise EPS'] < 10.:\n",
    "            return ['background-color: lightgreen']*4\n",
    "        elif s['Predicted Surprise EPS'] <= 0. and s['Predicted Surprise EPS'] > -10.:\n",
    "            return ['background-color: pink']*4\n",
    "        else:\n",
    "            return ['background-color: red']*4\n",
    "\n",
    "    filtered_X_test_predicted = X_test_predicted[X_test_predicted.Company.isin(company_list)]\n",
    "\n",
    "    buy_or_sell = []\n",
    "    for eps in filtered_X_test_predicted['Predicted Surprise EPS']:\n",
    "        if eps >= 10.:\n",
    "            buy_or_sell.append('Strong Buy')\n",
    "        elif eps > 0. and eps < 10.:\n",
    "            buy_or_sell.append('Weak Buy')\n",
    "        elif eps <= 0. and eps > -10.:\n",
    "            buy_or_sell.append('Weak Sell')\n",
    "        else:\n",
    "            buy_or_sell.append('Strong Sell')\n",
    "    filtered_X_test_predicted.loc[:,'Recommendation'] = buy_or_sell\n",
    "\n",
    "    release_date = []\n",
    "    for row in range(len(filtered_X_test_predicted)):\n",
    "        df_row = filtered_X_test_predicted.iloc[row, :]\n",
    "        company = df_row['Company']\n",
    "        date_range = df_row['Quarter Dates']\n",
    "        release_date.append(quarter_release_date[company][date_range])\n",
    "    filtered_X_test_predicted.loc[:,'Report Date'] = release_date\n",
    "\n",
    "    company_list.sort()\n",
    "    company_list.insert(0,'ALL COMPANIES')\n",
    "\n",
    "    dropdown_company = widgets.Dropdown(options = company_list)\n",
    "    dropdown_year = widgets.Dropdown(options = ['ALL YEARS', '2017', '2018'])\n",
    "    dropdown_quarter = widgets.Dropdown(options = quarter_date_list)\n",
    "\n",
    "    output = widgets.Output()\n",
    "\n",
    "    def date_quarter_mask(dataframe, month_list):\n",
    "        mask = []\n",
    "        for index in range(len(dataframe)):\n",
    "            row = dataframe.iloc[index]\n",
    "            if row['Report Date'].month in month_list:\n",
    "                mask.append(True)\n",
    "            else:\n",
    "                mask.append(False)\n",
    "        return mask\n",
    "\n",
    "    def date_year_mask(dataframe, year):\n",
    "        mask = []\n",
    "        for index in range(len(dataframe)):\n",
    "            row = dataframe.iloc[index]\n",
    "            if row['Report Date'].year == int(year):\n",
    "                mask.append(True)\n",
    "            else:\n",
    "                mask.append(False)\n",
    "        return mask\n",
    "\n",
    "    def quarter_year_mask(dataframe, year, month_list):\n",
    "        mask = []\n",
    "        for index in range(len(dataframe)):\n",
    "            row = dataframe.iloc[index]\n",
    "            if row['Report Date'].month in month_list and row['Report Date'].year == int(year):\n",
    "                mask.append(True)\n",
    "            else:\n",
    "                mask.append(False)\n",
    "        return mask\n",
    "\n",
    "    def common_filtering(company, year, quarter):\n",
    "        output.clear_output()\n",
    "        filtered_X_test_predicted.sort_values(by=['Report Date','Company'], inplace=True)\n",
    "\n",
    "        if (company == 'ALL COMPANIES') & (year == 'ALL YEARS') & (quarter == 'ALL QUARTERS'):\n",
    "            common_filter = filtered_X_test_predicted.loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "\n",
    "        elif (company == 'ALL COMPANIES') & (year == 'ALL YEARS') & (quarter != 'ALL QUARTERS'):\n",
    "            if quarter == 'Jan 1 - Mar 31':\n",
    "                mask = date_quarter_mask(filtered_X_test_predicted, [1, 2, 3])\n",
    "                common_filter = filtered_X_test_predicted[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "            elif quarter == 'Apr 1 - June 30':\n",
    "                mask = date_quarter_mask(filtered_X_test_predicted, [4, 5, 6])\n",
    "                common_filter = filtered_X_test_predicted[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "            elif quarter == 'Jul 1 - Sep 30':\n",
    "                mask = date_quarter_mask(filtered_X_test_predicted, [7, 8, 9])\n",
    "                common_filter = filtered_X_test_predicted[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "            elif quarter == 'Oct 1 - Dec 31':\n",
    "                mask = date_quarter_mask(filtered_X_test_predicted, [10, 11, 12])\n",
    "                common_filter = filtered_X_test_predicted[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "\n",
    "        elif (company == 'ALL COMPANIES') and (year != 'ALL YEARS') and (quarter == 'ALL QUARTERS'):\n",
    "            mask = date_year_mask(filtered_X_test_predicted, year)\n",
    "            common_filter = filtered_X_test_predicted[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "\n",
    "        elif (company == 'ALL COMPANIES') and (year != 'ALL YEARS') and (quarter != 'ALL QUARTERS'):     \n",
    "            if quarter == 'Jan 1 - Mar 31':\n",
    "                mask = quarter_year_mask(filtered_X_test_predicted, year, [1, 2, 3])\n",
    "                common_filter = filtered_X_test_predicted[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "            elif quarter == 'Apr 1 - June 30':\n",
    "                mask = quarter_year_mask(filtered_X_test_predicted, year, [4, 5, 6]) \n",
    "                common_filter = filtered_X_test_predicted[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "            elif quarter == 'Jul 1 - Sep 30':\n",
    "                mask = quarter_year_mask(filtered_X_test_predicted, year, [7, 8, 9]) \n",
    "                common_filter = filtered_X_test_predicted[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "            elif quarter == 'Oct 1 - Dec 31':\n",
    "                mask = quarter_year_mask(filtered_X_test_predicted, year, [10, 11, 12]) \n",
    "                common_filter = filtered_X_test_predicted[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "\n",
    "        elif (company != 'ALL COMPANIES') and (year == 'ALL YEARS') and (quarter == 'ALL QUARTERS'):\n",
    "            common_filter = filtered_X_test_predicted[filtered_X_test_predicted.Company == company] \\\n",
    "                        .loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "\n",
    "        elif (company != 'ALL COMPANIES') and (year == 'ALL YEARS') and (quarter != 'ALL QUARTERS'):\n",
    "            if quarter == 'Jan 1 - Mar 31':\n",
    "                tempdf = filtered_X_test_predicted[filtered_X_test_predicted.Company == company]\n",
    "                mask = date_quarter_mask(tempdf, [1, 2, 3])\n",
    "                common_filter = tempdf[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "            elif quarter == 'Apr 1 - June 30':\n",
    "                tempdf = filtered_X_test_predicted[filtered_X_test_predicted.Company == company]\n",
    "                mask = date_quarter_mask(tempdf, [4, 5, 6])\n",
    "                common_filter = tempdf[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "            elif quarter == 'Jul 1 - Sep 30':\n",
    "                tempdf = filtered_X_test_predicted[filtered_X_test_predicted.Company == company]\n",
    "                mask = date_quarter_mask(tempdf, [7, 8, 9])\n",
    "                common_filter = tempdf[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "            elif quarter == 'Oct 1 - Dec 31':\n",
    "                tempdf = filtered_X_test_predicted[filtered_X_test_predicted.Company == company]\n",
    "                mask = date_quarter_mask(tempdf, [10, 11, 12])\n",
    "                common_filter = tempdf[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "\n",
    "        elif (company != 'ALL COMPANIES') and (year != 'ALL YEARS') and (quarter == 'ALL QUARTERS'):\n",
    "            tempdf = filtered_X_test_predicted[filtered_X_test_predicted.Company == company]\n",
    "            mask = date_year_mask(tempdf, year)\n",
    "            common_filter = tempdf[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "\n",
    "        elif (company != 'ALL COMPANIES') and (year != 'ALL YEARS') and (quarter != 'ALL QUARTERS'):\n",
    "            tempdf = filtered_X_test_predicted[filtered_X_test_predicted.Company == company]\n",
    "            if quarter == 'Jan 1 - Mar 31':\n",
    "                mask = quarter_year_mask(tempdf, year, [1, 2, 3])\n",
    "                common_filter = tempdf[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "            elif quarter == 'Apr 1 - June 30':\n",
    "                mask = quarter_year_mask(tempdf, year, [4, 5, 6]) \n",
    "                common_filter = tempdf[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "            elif quarter == 'Jul 1 - Sep 30':\n",
    "                mask = quarter_year_mask(tempdf, year, [7, 8, 9]) \n",
    "                common_filter = tempdf[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "            elif quarter == 'Oct 1 - Dec 31':\n",
    "                mask = quarter_year_mask(tempdf, year, [10, 11, 12]) \n",
    "                common_filter = tempdf[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date', 'Predicted Surprise EPS', 'Recommendation']]\n",
    "\n",
    "        with output:\n",
    "            display(common_filter.sort_values(by=['Report Date','Company']) \\\n",
    "                    .style.apply(highlight_greaterthan_1, axis=1))\n",
    "\n",
    "    def dropdown_company_eventhandler(change):\n",
    "        common_filtering(change.new, dropdown_year.value, dropdown_quarter.value)\n",
    "    def dropdown_year_eventhandler(change):\n",
    "        common_filtering(dropdown_company.value, change.new, dropdown_quarter.value)\n",
    "    def dropdown_quarter_eventhandler(change):\n",
    "        common_filtering(dropdown_company.value, dropdown_year.value, change.new)\n",
    "\n",
    "    dropdown_company.observe(dropdown_company_eventhandler, names='value')\n",
    "    dropdown_year.observe(dropdown_year_eventhandler, names='value')\n",
    "    dropdown_quarter.observe(dropdown_quarter_eventhandler, names='value')\n",
    "\n",
    "    display(dropdown_company)\n",
    "    display(dropdown_year)\n",
    "    display(dropdown_quarter)\n",
    "\n",
    "    display(output)\n",
    "    pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_plot():\n",
    "    import pickle\n",
    "    from datetime import datetime\n",
    "    from IPython.display import display\n",
    "    from bokeh.io import output_file, show\n",
    "    from bokeh.layouts import gridplot\n",
    "    from bokeh.palettes import Viridis3\n",
    "    from bokeh.plotting import figure, output_notebook\n",
    "    from collections import OrderedDict\n",
    "    from bokeh.models import ColumnDataSource, LabelSet, Label\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from ipywidgets import widgets\n",
    "    \n",
    "#     with open('dowjones_date_quarter_price_dict.pickle', 'rb') as handle:\n",
    "#         dowjones_date_quarter_price_dict = pickle.load(handle)\n",
    "#     with open('select_profit.pickle', 'rb') as handle:\n",
    "#         select_profit = pickle.load(handle)\n",
    "#     with open('select_cost.pickle', 'rb') as handle:\n",
    "#         select_cost = pickle.load(handle)\n",
    "#     X_test_predicted = pd.read_pickle('X_test_predicted.pkl')\n",
    "#     with open('quarter_release_date.pickle', 'rb') as handle:\n",
    "#         quarter_release_date = pickle.load(handle)\n",
    "\n",
    "\n",
    "    company_list = ['WALMART',\n",
    "                        'TEXASROADHOUSE',\n",
    "                        'DENNYS',\n",
    "                        'DILLARDS',\n",
    "                        'BIGLOTS',\n",
    "                        'PLANETFITNESS',\n",
    "                        'BIG5SPORTINGGOODS',\n",
    "                        'LUMBERLIQUIDATORS',\n",
    "                        'CHIPOTLE',\n",
    "                        'DOLLARGENERAL',\n",
    "                        'REDROBIN',\n",
    "                        'DELTA',\n",
    "                        'DESTINATIONXL',\n",
    "                        'WINGSTOP',\n",
    "                        'SEAWORLD',\n",
    "                        'MCDONALDSUS',\n",
    "                        'NORWEGIANCRUISELINE',\n",
    "                        'ADVANCEAUTOPARTS',\n",
    "                        'CHILDRENSPLACE',\n",
    "                        'GUESS',\n",
    "                        'KROGER',\n",
    "                        'NORDSTROMRACK',\n",
    "                        'CAESARSENTERTAINMENTCORP',\n",
    "                        'ROYALCARIBBEAN',\n",
    "                        'FOOTLOCKER',\n",
    "                        'ESTEELAUDERCOMPANIES',\n",
    "                        'CRACKERBARREL',\n",
    "                        'AMERICANAIRLINES',\n",
    "                        'LAZBOY',\n",
    "                        'NIKE',\n",
    "                        'MARRIOTTINTERNATIONAL',\n",
    "                        'AUTONATION',\n",
    "                        'EXTENDEDSTAYAMERICA',\n",
    "                        'NATURALGROCERS',\n",
    "                        'SHAKESHACK',\n",
    "                        'POTBELLYSANDWICHSHOP',\n",
    "                        'KOHLS']\n",
    "\n",
    "    quarter_date_list = ['ALL QUARTERS', 'Jan 1 - Mar 31', 'Apr 1 - Jun 30', 'Jul 1 - Sep 30', 'Oct 1 - Dec 31']\n",
    "\n",
    "    def highlight_greaterthan_1(s):\n",
    "        if s['Predicted Surprise EPS'] >= 10.:\n",
    "            return ['background-color: green']*4\n",
    "        elif s['Predicted Surprise EPS'] > 0. and s['Predicted Surprise EPS'] < 10.:\n",
    "            return ['background-color: lightgreen']*4\n",
    "        elif s['Predicted Surprise EPS'] <= 0. and s['Predicted Surprise EPS'] > -10.:\n",
    "            return ['background-color: pink']*4\n",
    "        else:\n",
    "            return ['background-color: red']*4\n",
    "\n",
    "    filtered_X_test_predicted = X_test_predicted[X_test_predicted.Company.isin(company_list)]\n",
    "\n",
    "    buy_or_sell = []\n",
    "    for eps in filtered_X_test_predicted['Predicted Surprise EPS']:\n",
    "        if eps >= 10.:\n",
    "            buy_or_sell.append('Strong Buy')\n",
    "        elif eps > 0. and eps < 10.:\n",
    "            buy_or_sell.append('Weak Buy')\n",
    "        elif eps <= 0. and eps > -10.:\n",
    "            buy_or_sell.append('Weak Sell')\n",
    "        else:\n",
    "            buy_or_sell.append('Strong Sell')\n",
    "    filtered_X_test_predicted.loc[:,'Recommendation'] = buy_or_sell\n",
    "\n",
    "    release_date = []\n",
    "    for row in range(len(filtered_X_test_predicted)):\n",
    "        df_row = filtered_X_test_predicted.iloc[row, :]\n",
    "        company = df_row['Company']\n",
    "        date_range = df_row['Quarter Dates']\n",
    "        release_date.append(quarter_release_date[company][date_range])\n",
    "    filtered_X_test_predicted.loc[:,'Report Date'] = release_date\n",
    "\n",
    "    ###(select_cost, select_profit) = get_profit_per_release(filtered_X_test_predicted)\n",
    "\n",
    "    filtered_X_test_predicted.loc[:,'Cost'] = select_cost\n",
    "    filtered_X_test_predicted.loc[:,'Profit'] = select_profit\n",
    "\n",
    "    company_list.sort()\n",
    "    company_list.insert(0,'ALL COMPANIES')\n",
    "\n",
    "    dropdown_year = widgets.Dropdown(options = ['ALL YEARS', '2017', '2018'])\n",
    "    dropdown_quarter = widgets.Dropdown(options = quarter_date_list)\n",
    "\n",
    "    output = widgets.Output()\n",
    "\n",
    "    def date_quarter_mask(dataframe, month_list):\n",
    "        mask = []\n",
    "        for index in range(len(dataframe)):\n",
    "            row = dataframe.iloc[index]\n",
    "            if row['Report Date'].month in month_list:\n",
    "                mask.append(True)\n",
    "            else:\n",
    "                mask.append(False)\n",
    "        return mask\n",
    "\n",
    "    def date_year_mask(dataframe, year):\n",
    "        mask = []\n",
    "        for index in range(len(dataframe)):\n",
    "            row = dataframe.iloc[index]\n",
    "            if row['Report Date'].year == int(year):\n",
    "                mask.append(True)\n",
    "            else:\n",
    "                mask.append(False)\n",
    "        return mask\n",
    "\n",
    "    def quarter_year_mask(dataframe, year, month_list):\n",
    "        mask = []\n",
    "        for index in range(len(dataframe)):\n",
    "            row = dataframe.iloc[index]\n",
    "            if row['Report Date'].month in month_list and row['Report Date'].year == int(year):\n",
    "                mask.append(True)\n",
    "            else:\n",
    "                mask.append(False)\n",
    "        return mask\n",
    "\n",
    "    def get_yield(dataframe):\n",
    "\n",
    "        running_cost = 0\n",
    "        running_profit = 0\n",
    "        investment_yield = {}\n",
    "        for index in range(len(dataframe)):\n",
    "            row = dataframe.iloc[index]\n",
    "            cost = row['Cost']\n",
    "            profit = row['Profit']\n",
    "            date = row['Report Date']\n",
    "\n",
    "            running_cost += cost\n",
    "            running_profit += profit\n",
    "            temp_yield = running_profit * 100 / running_cost\n",
    "            investment_yield[date] = temp_yield\n",
    "\n",
    "        investment_yield1 = OrderedDict(sorted(investment_yield.items()))\n",
    "\n",
    "        return investment_yield1\n",
    "\n",
    "    def common_filtering(year, quarter):\n",
    "        output.clear_output()\n",
    "        filtered_X_test_predicted.sort_values(by=['Company', 'Report Date'], inplace=True)\n",
    "\n",
    "        if (year == 'ALL YEARS') & (quarter == 'ALL QUARTERS'):\n",
    "            common_filter = filtered_X_test_predicted \\\n",
    "                .loc[:, ['Company', 'Report Date', 'Cost', 'Profit']]\n",
    "            investment_yield = get_yield(common_filter)\n",
    "\n",
    "        elif (year != 'ALL YEARS') and (quarter == 'ALL QUARTERS'):\n",
    "            mask = date_year_mask(filtered_X_test_predicted, year)\n",
    "            common_filter = filtered_X_test_predicted[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date','Cost', 'Profit']]\n",
    "            investment_yield = get_yield(common_filter)\n",
    "\n",
    "        elif (year != 'ALL YEARS') and (quarter != 'ALL QUARTERS'):     \n",
    "            if quarter == 'Jan 1 - Mar 31':\n",
    "                mask = quarter_year_mask(filtered_X_test_predicted, year, [1, 2, 3])\n",
    "                common_filter = filtered_X_test_predicted[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date','Cost', 'Profit']]\n",
    "            elif quarter == 'Apr 1 - Jun 30':\n",
    "                mask = quarter_year_mask(filtered_X_test_predicted, year, [4, 5, 6]) \n",
    "                common_filter = filtered_X_test_predicted[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date','Cost', 'Profit']]\n",
    "            elif quarter == 'Jul 1 - Sep 30':\n",
    "                mask = quarter_year_mask(filtered_X_test_predicted, year, [7, 8, 9]) \n",
    "                common_filter = filtered_X_test_predicted[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date','Cost', 'Profit']]\n",
    "            elif quarter == 'Oct 1 - Dec 31':\n",
    "                mask = quarter_year_mask(filtered_X_test_predicted, year, [10, 11, 12]) \n",
    "                common_filter = filtered_X_test_predicted[mask] \\\n",
    "                        .loc[:, ['Company', 'Report Date','Cost', 'Profit']]\n",
    "            investment_yield = get_yield(common_filter)\n",
    "\n",
    "        with output:\n",
    "\n",
    "            if (year != 'ALL YEARS') and (quarter != 'ALL QUARTERS'):\n",
    "                xdatastr = dowjones_date_quarter_price_dict[(year, quarter)].keys()\n",
    "                xdata = [datetime.strptime(i, '%Y-%m-%d').date() for i in xdatastr]\n",
    "                ypricedata = list(dowjones_date_quarter_price_dict[(year, quarter)].values())\n",
    "                ydata = [(i-ypricedata[-1])*100/ypricedata[-1] for i in ypricedata]\n",
    "\n",
    "            elif (year != 'ALL YEARS') and (quarter == 'ALL QUARTERS'):\n",
    "\n",
    "                q1_xdatastr = dowjones_date_quarter_price_dict[(year, 'Jan 1 - Mar 31')].keys()\n",
    "                q2_xdatastr = dowjones_date_quarter_price_dict[(year, 'Apr 1 - Jun 30')].keys()\n",
    "                q3_xdatastr = dowjones_date_quarter_price_dict[(year, 'Jul 1 - Sep 30')].keys()\n",
    "                q4_xdatastr = dowjones_date_quarter_price_dict[(year, 'Oct 1 - Dec 31')].keys()\n",
    "\n",
    "                q1_xdata = [datetime.strptime(i, '%Y-%m-%d').date() for i in q1_xdatastr]\n",
    "                q2_xdata = [datetime.strptime(i, '%Y-%m-%d').date() for i in q2_xdatastr]\n",
    "                q3_xdata = [datetime.strptime(i, '%Y-%m-%d').date() for i in q3_xdatastr]\n",
    "                q4_xdata = [datetime.strptime(i, '%Y-%m-%d').date() for i in q4_xdatastr]\n",
    "\n",
    "                q1_ypricedata = list(dowjones_date_quarter_price_dict[(year, 'Jan 1 - Mar 31')].values())\n",
    "                q2_ypricedata = list(dowjones_date_quarter_price_dict[(year, 'Apr 1 - Jun 30')].values())\n",
    "                q3_ypricedata = list(dowjones_date_quarter_price_dict[(year, 'Jul 1 - Sep 30')].values())\n",
    "                q4_ypricedata = list(dowjones_date_quarter_price_dict[(year, 'Oct 1 - Dec 31')].values())\n",
    "\n",
    "                q1_ydata = [(i-q1_ypricedata[-1])*100/q1_ypricedata[-1] for i in q1_ypricedata]\n",
    "                q2_ydata = [(i-q1_ypricedata[-1])*100/q1_ypricedata[-1] for i in q2_ypricedata]\n",
    "                q3_ydata = [(i-q1_ypricedata[-1])*100/q1_ypricedata[-1] for i in q3_ypricedata]\n",
    "                q4_ydata = [(i-q1_ypricedata[-1])*100/q1_ypricedata[-1] for i in q4_ypricedata]\n",
    "\n",
    "                xdata = q4_xdata + q3_xdata + q2_xdata + q1_xdata\n",
    "                ydata = q4_ydata + q3_ydata + q2_ydata + q1_ydata\n",
    "\n",
    "            elif (year == 'ALL YEARS') and (quarter == 'ALL QUARTERS'):\n",
    "                xdata = []\n",
    "                ydata = []\n",
    "                for y in ['2018', '2017']:\n",
    "                    q1_xdatastr = dowjones_date_quarter_price_dict[(y, 'Jan 1 - Mar 31')].keys()\n",
    "                    q2_xdatastr = dowjones_date_quarter_price_dict[(y, 'Apr 1 - Jun 30')].keys()\n",
    "                    q3_xdatastr = dowjones_date_quarter_price_dict[(y, 'Jul 1 - Sep 30')].keys()\n",
    "                    q4_xdatastr = dowjones_date_quarter_price_dict[(y, 'Oct 1 - Dec 31')].keys()\n",
    "\n",
    "                    q1_xdata = [datetime.strptime(i, '%Y-%m-%d').date() for i in q1_xdatastr]\n",
    "                    q2_xdata = [datetime.strptime(i, '%Y-%m-%d').date() for i in q2_xdatastr]\n",
    "                    q3_xdata = [datetime.strptime(i, '%Y-%m-%d').date() for i in q3_xdatastr]\n",
    "                    q4_xdata = [datetime.strptime(i, '%Y-%m-%d').date() for i in q4_xdatastr]\n",
    "\n",
    "                    q1_ypricedata = list(dowjones_date_quarter_price_dict[(y, 'Jan 1 - Mar 31')].values())\n",
    "\n",
    "                    fixed_start = list(dowjones_date_quarter_price_dict[('2017', 'Jan 1 - Mar 31')].values())\n",
    "                    q2_ypricedata = list(dowjones_date_quarter_price_dict[(y, 'Apr 1 - Jun 30')].values())\n",
    "                    q3_ypricedata = list(dowjones_date_quarter_price_dict[(y, 'Jul 1 - Sep 30')].values())\n",
    "                    q4_ypricedata = list(dowjones_date_quarter_price_dict[(y, 'Oct 1 - Dec 31')].values())\n",
    "\n",
    "                    q1_ydata = [(i-fixed_start[-1])*100/fixed_start[-1] for i in q1_ypricedata]\n",
    "                    q2_ydata = [(i-fixed_start[-1])*100/fixed_start[-1] for i in q2_ypricedata]\n",
    "                    q3_ydata = [(i-fixed_start[-1])*100/fixed_start[-1] for i in q3_ypricedata]\n",
    "                    q4_ydata = [(i-fixed_start[-1])*100/fixed_start[-1] for i in q4_ypricedata]\n",
    "\n",
    "                    xdata += q4_xdata + q3_xdata + q2_xdata + q1_xdata\n",
    "                    ydata += q4_ydata + q3_ydata + q2_ydata + q1_ydata\n",
    "\n",
    "            output_notebook()\n",
    "\n",
    "            p = figure(\n",
    "               tools=['pan','box_zoom','reset','save'],\n",
    "                title=\"Machine Learning and Naive Model Portfolio Growth\",\n",
    "               x_axis_label='Date', y_axis_label='Portfolio Growth (% Change)',\n",
    "                x_axis_type=\"datetime\" ,\n",
    "            )\n",
    "            p.width = 400\n",
    "            p.height = 400\n",
    "\n",
    "            p.line(xdata, ydata, color='black', legend='Dow Jones Index Fund (Naive)')\n",
    "            MLxdata = list(investment_yield.keys())\n",
    "            MLydata = list(investment_yield.values())\n",
    "\n",
    "            p.line(MLxdata, MLydata, legend='ML Predictor Model', color='red')\n",
    "            p.legend.location = 'top_left'\n",
    "\n",
    "            p2 = figure()\n",
    "            p2.width = 400\n",
    "            p2.height = 400\n",
    "            try:\n",
    "                names = ['ML Predictor Investment Return:', \n",
    "                                               '{}%'.format(np.round(MLydata[-1], decimals=2)), \n",
    "                                               'Naive Investment Return:', \n",
    "                                               '{}%'.format(np.round(ydata[0], decimals=2))]\n",
    "            except:\n",
    "                print('No data for this selection')\n",
    "                names = ['ML Predictor Investment Return:', \n",
    "                                               '{}%'.format(0, decimals=2), \n",
    "                                               'Naive Investment Return:', \n",
    "                                               '{}%'.format(np.round(ydata[0], decimals=2))]\n",
    "                \n",
    "            source = ColumnDataSource(data=dict(height=[250, 200, 100, 50],\n",
    "                                        weight=[45, 160, 75, 160],\n",
    "                                        names=names))\n",
    "            labels = LabelSet(x='weight', y='height', text='names', level='glyph',\n",
    "                  x_offset=5, y_offset=5, source=source, render_mode='canvas',\n",
    "                             x_units='screen', y_units='screen',text_font_size=\"15pt\")\n",
    "\n",
    "            p2.add_layout(labels)\n",
    "            grid = gridplot([[p, p2]])\n",
    "            show(grid)\n",
    "\n",
    "\n",
    "    def dropdown_year_eventhandler(change):\n",
    "        common_filtering(change.new, dropdown_quarter.value)\n",
    "    def dropdown_quarter_eventhandler(change):\n",
    "        common_filtering(dropdown_year.value, change.new)\n",
    "\n",
    "    dropdown_year.observe(dropdown_year_eventhandler, names='value')\n",
    "    dropdown_quarter.observe(dropdown_quarter_eventhandler, names='value')\n",
    "\n",
    "    display(dropdown_year)\n",
    "    display(dropdown_quarter)\n",
    "\n",
    "    display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/jovyan/temp_datalab_records_social_facebook.csv', parse_dates=True, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(dataframe):\n",
    "    \"\"\"Deletes unwanted columns from dataframe,\n",
    "    Converts date column to datetime.date object\n",
    "    Strips and converts username column to uppercase\"\"\"\n",
    "    \n",
    "    dataframe.time = pd.to_datetime(dataframe.time, format='%Y-%m-%d')\n",
    "    dataframe.time = [x.date() for x in dataframe.time]\n",
    "    \n",
    "    del dataframe['date_added']\n",
    "    del dataframe['date_updated']\n",
    "    del dataframe['entity_id']\n",
    "    del dataframe['cusip']\n",
    "    del dataframe['isin']\n",
    "    del dataframe['has_added_app']\n",
    "    del dataframe['facebook_id']\n",
    "    del dataframe['were_here_count']\n",
    "    \n",
    "    dataframe.username = dataframe.username.str.upper()\n",
    "    dataframe.username = dataframe.username.str.strip()\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = clean_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_unique_companies(dataframe):\n",
    "    \"\"\"Generates list of unique companies in dataframe\"\"\"\n",
    "    clist = list(dataframe.username.unique())\n",
    "    while np.nan in clist:\n",
    "        clist.remove(np.nan)\n",
    "    return clist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_companies = get_unique_companies(dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_physical_companies(dataframe, company_list):\n",
    "    \"\"\"Removes companies from company_list where \n",
    "    the max or min checkins is equal to 0, or where \n",
    "    the checkins have not changes, which is indicative of\n",
    "    a non-physical company\"\"\"\n",
    "    physical_company = []\n",
    "    \n",
    "    for company in company_list:\n",
    "        \n",
    "        df = dataframe[dataframe.username == company]\n",
    "        if (np.max(df.checkins) == 0) or (np.min(df.checkins) == 0):\n",
    "            pass\n",
    "        elif np.max(df.checkins) == np.min(df.checkins):\n",
    "            pass\n",
    "        else:\n",
    "            physical_company.append(company)\n",
    "    return physical_company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_companies = isolate_physical_companies(dff, facebook_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('physical_companies.pickle', 'wb') as handle:\n",
    "#     pickle.dump(physical_companies, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('physical_companies.pickle', 'rb') as handle:\n",
    "    physical_companies = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process \n",
    "\n",
    "def best_name_match(fname, stock_names):\n",
    "    \"\"\"Given a list of possible company names matching the company name\n",
    "    finds the best match \"\"\"\n",
    "    \n",
    "    result = process.extractOne(fname, stock_names)\n",
    "\n",
    "    return [result[0], result[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_info(name):\n",
    "    \"\"\"Uses the Intrinio financial API to search for companies sounding similar to \n",
    "    the given company. Will return actual company name, ticker, and % certainty\"\"\"\n",
    "    import requests\n",
    "    import json\n",
    "    import time\n",
    "    \n",
    "    names = []\n",
    "    ticker = []\n",
    "    \n",
    "    def make_page_request():\n",
    "        url = 'https://api-v2.intrinio.com/companies/search?query={}'.format(name)\n",
    "        page = requests.get(url, params={'api_key':'Ojg5ZDFmOGNmMzJiOWZjM2RjZGNhNDRiM2JiNWJkM2M0'})\n",
    "        return page\n",
    "    \n",
    "    status_code = 0\n",
    "    while status_code != 200:\n",
    "        page = make_page_request()\n",
    "        status_code = page.status_code\n",
    "        if status_code != 200:\n",
    "            print('Failed with status code {}, retrying'.format(status_code))\n",
    "            time.sleep(5)\n",
    "        \n",
    "    best_match = json.loads(page.text)\n",
    "\n",
    "    for comp in best_match['companies']:\n",
    "        names.append(comp['name'])\n",
    "        ticker.append(comp['ticker'])\n",
    "\n",
    "    if names:\n",
    "        result = best_name_match(name, names)\n",
    "    else:\n",
    "        return {'name':np.nan, 'ticker':np.nan, 'certainty':np.nan}\n",
    "\n",
    "    #return {'name':names[result[0]], 'ticker':ticker[result[0]], 'certainty':result[1]}\n",
    "    return {'name':names[0], 'ticker':ticker[0], 'certainty':result[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_all_companies(company_list):\n",
    "    \"\"\"For each comany in company_list, get actual company name\n",
    "    and stock ticker\"\"\"\n",
    "    \n",
    "    import time\n",
    "    company_master_list = {}\n",
    "    \n",
    "    for company in company_list:\n",
    "        \n",
    "        print(company)\n",
    "        time.sleep(0.51)\n",
    "        result = get_stock_info(company)\n",
    "        \n",
    "        if result['name'] != np.nan:\n",
    "            company_master_list[company] = result\n",
    "            \n",
    "    return company_master_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_date_ranges(dataframe, company_list, company_ticker_dict):\n",
    "    \"\"\"Finds earliest and latest date for each company\"\"\"\n",
    "    \n",
    "    for company in company_list:\n",
    "        df = dataframe[dataframe.username == company]\n",
    "        min_date = np.min(df.time)\n",
    "        max_date = np.max(df.time)\n",
    "        company_ticker_dict[company]['Earliest Date'] = min_date\n",
    "        company_ticker_dict[company]['Latest Date'] = max_date\n",
    "        \n",
    "    return company_ticker_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_ticker_dict = get_company_date_ranges(dff, physical_companies, company_ticker_dict.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('company_ticker_dict.pickle', 'wb') as handle:\n",
    "#     pickle.dump(company_ticker_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('company_ticker_dict.pickle', 'rb') as handle:\n",
    "    company_ticker_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "import time \n",
    "\n",
    "def get_yahoo_finance_page(ticker):\n",
    "    \"\"\"Gets yahoo finance page containing EPS data\"\"\"\n",
    "    #https://finance.yahoo.com/calendar/earnings?day=2019-10-09&symbol=GE\n",
    "    \n",
    "    now = datetime.now()\n",
    "    year = now.year\n",
    "    month = now.month\n",
    "    day = now.day\n",
    "    \n",
    "    base = 'https://finance.yahoo.com/calendar/earnings?day='\n",
    "    url = '{}{}-{:02d}-{:02d}&symbol={}'.format(base,year, month, day, ticker)\n",
    "    print(url)\n",
    "    \n",
    "    status_code = 0\n",
    "    fault_counter = 0\n",
    "    \n",
    "    while status_code != 200:        \n",
    "        page = requests.get(url)\n",
    "        status_code = page.status_code\n",
    "        if status_code != 200:\n",
    "            fault_counter += 1\n",
    "            time.sleep(5)\n",
    "        if fault_counter > 5:\n",
    "            return None\n",
    "        \n",
    "    return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta  \n",
    "\n",
    "def get_table_on_page(page, min_date, max_date):\n",
    "    \"\"\"Parses EPS data from table\"\"\"\n",
    "    \n",
    "    import re\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    earnings_table = {}\n",
    "    earnings_table_sorted = {}\n",
    "    date_list = []\n",
    "    estimate = []\n",
    "    reported = []\n",
    "    surprise = []\n",
    "    \n",
    "    \n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    table = soup.findAll('tbody')\n",
    "    \n",
    "    if table:\n",
    "        table = table[0]\n",
    "        quarters = table.findAll('tr')\n",
    "        counter = 1\n",
    "        for quarter in quarters:\n",
    "            ED = quarter.findAll('td', attrs={'aria-label':'Earnings Date'})[0].text\n",
    "            temp = re.match('([A-Za-z]+\\s\\d+,\\s[0-9]{4})',ED)[1]\n",
    "            ED = datetime.strptime(temp, '%b %d, %Y').date()\n",
    "            #print(min_date, ED, max_date)\n",
    "\n",
    "            if ED > min_date and ED < max_date + timedelta(days=60):  #ED < max_date\n",
    "                earnings_table[counter] = {} \n",
    "                earnings_table[counter]['Report Date'] = ED\n",
    "                EPS = quarter.findAll('td', attrs={'aria-label':'EPS Estimate'})[0].text\n",
    "                if EPS == 'N/A' or EPS == '-':\n",
    "                    EPS = 0.\n",
    "                earnings_table[counter]['EPS Estimate'] = float(EPS)\n",
    "\n",
    "                EPS = quarter.findAll('td', attrs={'aria-label':'Reported EPS'})[0].text\n",
    "                if EPS == 'N/A' or EPS == '-':\n",
    "                    EPS = 0.\n",
    "                earnings_table[counter]['EPS Reported'] = float(EPS)\n",
    "\n",
    "                EPS = quarter.findAll('td', attrs={'aria-label':'Surprise(%)'})[0].text\n",
    "                if EPS == 'N/A' or EPS == '-':\n",
    "                    EPS = 0.\n",
    "                earnings_table[counter]['% Deviation'] = float(EPS)\n",
    "                counter += 1\n",
    "                \n",
    "        for i in range(len(earnings_table), 0, -1):\n",
    "\n",
    "            date_list.append(earnings_table[i]['Report Date'])\n",
    "            estimate.append(earnings_table[i]['EPS Estimate'])\n",
    "            reported.append(earnings_table[i]['EPS Reported'])\n",
    "            surprise.append(earnings_table[i]['% Deviation'])\n",
    "        \n",
    "        return {'date': date_list, 'estimate':estimate, 'reported':reported, 'surprise':surprise}\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def pull_EPS_data(company_dict):\n",
    "    \"\"\"Pulls EPS data out of table\"\"\"\n",
    "    dlist = {}\n",
    "    for company, v in company_dict.items():\n",
    "        ticker = v['ticker']\n",
    "        min_date = v['Earliest Date']\n",
    "        max_date = v['Latest Date']\n",
    "        print(company, ticker)\n",
    "        time.sleep(0.51)\n",
    "        page = get_yahoo_finance_page(ticker)\n",
    "        table = get_table_on_page(page, min_date, max_date)\n",
    "        #print(table)\n",
    "        if table:\n",
    "            dlist[company] = table\n",
    "    return dlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_data = pull_EPS_data(company_ticker_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('EPS_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(EPS_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('EPS_data.pickle', 'rb') as handle:\n",
    "#     EPS_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_price_from_ticker(ticker, start_date, end_date):\n",
    "    from datetime import datetime\n",
    "    from datetime import date\n",
    "    import time\n",
    "    import requests\n",
    "    import json\n",
    "    \n",
    "    base = 'https://api-v2.intrinio.com/securities/{}/prices'.format(ticker)\n",
    "    start_date = start_date.strftime('%Y-%m-%d')\n",
    "    end_date = end_date.strftime('%Y-%m-%d')\n",
    "    #print(start_date, end_date)\n",
    "    \n",
    "    params1={'api_key':'Ojg5ZDFmOGNmMzJiOWZjM2RjZGNhNDRiM2JiNWJkM2M0',\n",
    "                                    'start_date': start_date,\n",
    "                                    'end_date': end_date,\n",
    "                                     'frequency': 'daily',\n",
    "                                    'page_size': 10000}\n",
    "    \n",
    "    def run_request(params):\n",
    "        time.sleep(0.51)\n",
    "        page = requests.get(base, params)\n",
    "        return page\n",
    "    \n",
    "    status_code = 0\n",
    "    limit = 0\n",
    "    \n",
    "    while status_code != 200:\n",
    "        page = run_request(params1)\n",
    "        status_code = page.status_code\n",
    "        if status_code != 200:\n",
    "            limit += 1\n",
    "            time.sleep(5)\n",
    "        if limit > 5:\n",
    "            print('tried request 5+ times, breaking')\n",
    "            return None\n",
    "    #print(page.status_code)\n",
    "\n",
    "    result = json.loads(page.text)\n",
    "    if result:\n",
    "        result = result['stock_prices']\n",
    "        result.reverse()\n",
    "        return result\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def fill_in_stock_price(dataframe, company_dict):\n",
    "    \"\"\"Uses intrinio API to get EOD stock price for each company, adding that data to the dataframe\"\"\"\n",
    "\n",
    "    company_date_price_dict = {}\n",
    "    \n",
    "    for company, values in company_dict.items():\n",
    "        print(company)\n",
    "        \n",
    "        ticker = values['ticker']\n",
    "        min_date = values['Earliest Date']\n",
    "        max_date = values['Latest Date']\n",
    "        \n",
    "        if ticker == None:\n",
    "            continue\n",
    "            \n",
    "        df = dataframe[dataframe.username == company]\n",
    "        date_list = list(df.time)\n",
    "        date_list.sort(reverse=True)\n",
    "        #print(np.min(date_list), np.max(date_list))\n",
    "        \n",
    "        result = get_stock_price_from_ticker(ticker, min_date, max_date)\n",
    "        \n",
    "        if not result:\n",
    "            continue\n",
    "        \n",
    "        api_date_list = []\n",
    "        stock_price_list = []\n",
    "        \n",
    "        for day_stats in result:\n",
    "            date1 = datetime.strptime(day_stats['date'], '%Y-%m-%d').date()          \n",
    "            stock_price_list.append(day_stats['close'])\n",
    "            api_date_list.append(date1)\n",
    "        \n",
    "        api_date_list2 = []\n",
    "        stock_price_list2 = []\n",
    "        for date in date_list:\n",
    "            if date in api_date_list:\n",
    "                index = api_date_list.index(date)\n",
    "                stock_price_list2.append(stock_price_list[index])\n",
    "                api_date_list2.append(date)\n",
    "            else:\n",
    "                stock_price_list2.append(np.nan)\n",
    "                api_date_list2.append(date)\n",
    "            \n",
    "        company_date_price_dict[company] = {}\n",
    "        company_date_price_dict[company]['time'] = api_date_list2\n",
    "        company_date_price_dict[company]['price'] = stock_price_list2\n",
    "    \n",
    "    return company_date_price_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_date_price_dict = fill_in_stock_price(dff.copy(), company_ticker_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('company_date_price_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(company_date_price_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('company_date_price_dict.pickle', 'rb') as handle:\n",
    "#     company_date_price_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import simplejson as json\n",
    "import time\n",
    "def find_true_financial_quarter_results(ticker, date1):\n",
    "    \n",
    "    #print(date1)\n",
    "    year = date1.year\n",
    "    month = date1.month\n",
    "    day = date1.day\n",
    "    \n",
    "    #guess quarter\n",
    "    if month in [1, 2, 3]:\n",
    "        quarter = 1\n",
    "    elif month in [4, 5, 6]:\n",
    "        quarter = 2\n",
    "    elif month in [7, 8, 9]:\n",
    "        quarter = 3\n",
    "    elif month in [10, 11, 12]:\n",
    "        quarter = 4\n",
    "                        \n",
    "    \n",
    "    def make_request(url):    \n",
    "        status = 0\n",
    "        retry = 0\n",
    "        while status != 200:\n",
    "            time.sleep(0.51)\n",
    "            page = requests.get(url, params={'api_key':'Ojg5ZDFmOGNmMzJiOWZjM2RjZGNhNDRiM2JiNWJkM2M0'})\n",
    "            status = page.status_code\n",
    "            retry += 1\n",
    "            if retry == 4:\n",
    "                time.sleep(4)\n",
    "            if retry > 5:\n",
    "                print('Retry failed 5 times with code ', status)\n",
    "                return None\n",
    "        return page\n",
    "    \n",
    "    #check proper quarter\n",
    "    bounce_back = 0\n",
    "    while True:\n",
    "        financial_string = '{}-{}-{}-Q{}'.format(ticker, 'income_statement', year, quarter)\n",
    "        url = 'https://api-v2.intrinio.com/fundamentals/{}/standardized_financials'.format(financial_string)\n",
    "        page = make_request(url)  \n",
    "        if page is None:\n",
    "            return None\n",
    "        result = json.loads(page.text)\n",
    "        \n",
    "        if result:\n",
    "            check_quarter =  int(result['fundamental']['fiscal_period'][1])\n",
    "            start_date = result['fundamental']['start_date']\n",
    "            end_date = result['fundamental']['end_date']\n",
    "            start_date = datetime.strptime(start_date,'%Y-%m-%d').date()\n",
    "            end_date = datetime.strptime(end_date,'%Y-%m-%d').date()\n",
    "\n",
    "            if date1 > end_date:\n",
    "                quarter += 1\n",
    "                bounce_back += 1\n",
    "            elif date1 < start_date:\n",
    "                quarter -= 1\n",
    "                bounce_back += 1\n",
    "            else:\n",
    "                #print(result['standardized_financials'])\n",
    "                features = ['totalrevenue', 'totalgrossprofit', 'totaloperatingexpenses']\n",
    "\n",
    "                feature_dict = {}\n",
    "                for financial in result['standardized_financials']:\n",
    "                    if financial['data_tag']['tag'] in features:\n",
    "                        name = financial['data_tag']['name']\n",
    "                        value = financial['value']\n",
    "                        feature_dict[name] = value\n",
    "\n",
    "                #print(result['fundamental'])\n",
    "                #feature_dict['start_date']= result['fundamental']['start_date']\n",
    "                #feature_dict['end_date']= result['fundamental']['end_date']\n",
    "                \n",
    "                break\n",
    "                \n",
    "            if bounce_back > 2:\n",
    "                date1 += timedelta(days=30)\n",
    "            if quarter > 4:\n",
    "                quarter = 1\n",
    "                year += 1\n",
    "            if quarter < 1:\n",
    "                quarter = 4\n",
    "                year -= 1\n",
    "            #print(quarter, date1)\n",
    "            #print(start_date, end_date)\n",
    "\n",
    "            \n",
    "            \n",
    "    return [quarter, start_date, end_date, feature_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta  \n",
    "def get_company_quarters(company_dict):\n",
    "    \n",
    "    \n",
    "    company_quarters = {}\n",
    "    counter = 1\n",
    "    for company, value in company_dict.items():\n",
    "        \n",
    "        print('{}   {}/{}'.format(company, counter, len(company_dict)))\n",
    "        counter += 1\n",
    "        min_date = value['Earliest Date']\n",
    "        max_date = value['Latest Date']\n",
    "        ticker = value['ticker']\n",
    "        #print(ticker, min_date, max_date)\n",
    "        \n",
    "        if ticker != None:\n",
    "            \n",
    "            company_quarters[company] = {}\n",
    "            while min_date < max_date:\n",
    "                result = find_true_financial_quarter_results(ticker, min_date)\n",
    "                #print(result)\n",
    "                if result:\n",
    "                    [quarter, start_date, end_date, feature_dict] = result\n",
    "                    company_quarters[company][(start_date, end_date)] = {}\n",
    "                    company_quarters[company][(start_date, end_date)]['features'] = feature_dict\n",
    "                    min_date = end_date + timedelta(days=25)\n",
    "                else:\n",
    "                    break\n",
    "            #break\n",
    "        \n",
    "    return company_quarters\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_quarter_range = get_company_quarters(company_ticker_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('company_quarter_range.pickle', 'wb') as handle:\n",
    "    pickle.dump(company_quarter_range, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('company_quarter_range.pickle', 'rb') as handle:\n",
    "#     company_quarter_range = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_quarter_to_eps_release(EPS_data, quarter_data):\n",
    "    \"\"\"Find date closest to end of a quarter which corresponds with the release of EPS\"\"\"\n",
    "    company_quarter_eps_link = {}\n",
    "    counter = 1\n",
    "    for company, value_dict in EPS_data.items():\n",
    "        print('{}   {}/{}'.format(company, counter, len(EPS_data)))\n",
    "        counter += 1\n",
    "        if value_dict:\n",
    "            eps_releases = value_dict['date']\n",
    "            quarter_dates = quarter_data[company].keys()\n",
    "            begin_quarter_dates = [i[0] for i in quarter_dates]\n",
    "            end_quarter_dates = [i[1] for i in quarter_dates]\n",
    "\n",
    "            #print(eps_releases, '\\n')\n",
    "            #print(quarter_dates, '\\n')\n",
    "            #print(end_quarter_dates)\n",
    "            if eps_releases and end_quarter_dates:\n",
    "                company_quarter_eps_link[company] = {}\n",
    "                for index, current_eps_release_date in enumerate(eps_releases):\n",
    "                    distance_from = [(current_eps_release_date - i).days for i in end_quarter_dates]\n",
    "                    #print(distance_from)\n",
    "                    closest_index = -1\n",
    "                    for i, num in enumerate(distance_from):\n",
    "                        if num >= 0:\n",
    "                            closest_index = i\n",
    "                    #print(closest_index)\n",
    "                    #print(end_quarter_dates)\n",
    "                    #print(end_quarter_dates[closest_index], current_eps_release_date)\n",
    "                    if closest_index > -1:\n",
    "                        company_quarter_eps_link[company][(begin_quarter_dates[closest_index],end_quarter_dates[closest_index])] \\\n",
    "                                                           = current_eps_release_date\n",
    "                #print(company_quarter_eps_link[company])   \n",
    "        #break\n",
    "        \n",
    "    return company_quarter_eps_link\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter_release_date = link_quarter_to_eps_release(EPS_data, company_quarter_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('quarter_release_date.pickle', 'wb') as handle:\n",
    "    pickle.dump(quarter_release_date, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('quarter_release_date.pickle', 'rb') as handle:\n",
    "#     quarter_release_date = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "\n",
    "def generate_master_dataframe(quarter_release_dates, company_quarter_ranges, EPSdata, dataframe):\n",
    "    \"\"\"Constructs dataframe for each quarter, with avg likes, checkins, talking about count\n",
    "    with revenue, profit, expenses, EPS estimate, and finally EPS surprise. Feature engineering is\n",
    "    used to find ratios between profit over revenue and expenses over revenue\"\"\"\n",
    "\n",
    "    \n",
    "#     new_df = pd.DataFrame({'Company': [], 'Quarter Dates': [], 'Avg Checkins': [],\n",
    "#                           'Avg Likes': [], 'Avg TAC': [], 'Profit/Revenue': [], \n",
    "#                           'Expenses/Revenue': [], 'EPS': [], 'Surprise': []})\n",
    "    \n",
    "    counter = 1\n",
    "    for company, value_dict in quarter_release_dates.items():\n",
    "        \n",
    "        usernames = []\n",
    "        avg_checkins = []\n",
    "        avg_likes = []\n",
    "        avg_tac = []\n",
    "        revenue = []\n",
    "        profit = []\n",
    "        expenses = []\n",
    "        estimated_EPS = []\n",
    "        surprise = []\n",
    "        quarter_dates = []\n",
    "        \n",
    "        print('{}   {}/{}'.format(company, counter, len(quarter_release_dates)))\n",
    "        counter += 1\n",
    "               \n",
    "        quarter_dates = []\n",
    "        for qstart_end, epsrelease in value_dict.items():\n",
    "            usernames.append(company)\n",
    "            quarter_dates.append(qstart_end)\n",
    "            \n",
    "            tempdf = dataframe[dataframe.username == company]\n",
    "            tempdf = tempdf[tempdf.time >= qstart_end[0]]\n",
    "            tempdf = tempdf[tempdf.time < qstart_end[1]]\n",
    "            \n",
    "            avg_checkins.append(np.mean(tempdf.checkins))\n",
    "            avg_likes.append(np.mean(tempdf.likes))\n",
    "            avg_tac.append(np.mean(tempdf.talking_about_count))\n",
    "            \n",
    "        print('avg_checkins', avg_checkins)\n",
    "        print('avg_likes', avg_likes)\n",
    "        print('avg_tac', avg_tac)\n",
    "        print('usernames', usernames)\n",
    "        print('quarter_dates', quarter_dates)\n",
    "        \n",
    "        eps_dates = EPSdata[company]['date']\n",
    "        eps_estimate = EPSdata[company]['estimate']\n",
    "        eps_reported = EPSdata[company]['reported']\n",
    "        eps_surprise = EPSdata[company]['surprise']\n",
    "        \n",
    "        print('eps_dates', eps_dates)\n",
    "        print('eps_estimate', eps_estimate)\n",
    "        print('eps_reported', eps_reported)\n",
    "        print('eps_surprises', eps_surprise)\n",
    "                   \n",
    "        temp_dates = []\n",
    "        for qstart_end, feature in company_quarter_ranges[company].items():\n",
    "            financial_features = feature['features'].keys()\n",
    "            if (qstart_end in quarter_dates) and \\\n",
    "                ('Total Revenue' in financial_features) and \\\n",
    "                ('Total Gross Profit' in financial_features) and \\\n",
    "                ('Total Operating Expenses' in financial_features):\n",
    "                temp_dates.append(qstart_end)\n",
    "                revenue.append(feature['features']['Total Revenue'])\n",
    "                profit.append(feature['features']['Total Gross Profit'])\n",
    "                expenses.append(feature['features']['Total Operating Expenses'])\n",
    "        \n",
    "        print(temp_dates)\n",
    "        print(revenue)\n",
    "        print(profit)\n",
    "        print(expenses)\n",
    "        \n",
    "        profit_over_revenue = [i/j for i, j in zip(profit, revenue)]\n",
    "        expenses_over_revenue = [i/j for i, j in zip(expenses, revenue)]\n",
    "        print('profit_over_revenue', profit_over_revenue)\n",
    "        print('expenses_over_revenue', expenses_over_revenue)\n",
    "        \n",
    "        len_usernames = len(usernames)\n",
    "        len_quarter_dates = len(quarter_dates)\n",
    "        len_avg_checkins = len(avg_checkins)\n",
    "        len_avg_likes = len(avg_likes)\n",
    "        len_avg_tac = len(avg_tac)\n",
    "        len_profit_over_revenue = len(profit_over_revenue)\n",
    "        len_expenses_over_revenue = len(expenses_over_revenue)\n",
    "        len_eps_reported = len(eps_reported)\n",
    "        len_eps_surprise = len(eps_surprise)\n",
    "        \n",
    "        min_entries = np.min([len_usernames, len_quarter_dates, len_avg_checkins,\n",
    "                           len_avg_likes, len_avg_tac, len_profit_over_revenue, \n",
    "                           len_expenses_over_revenue, len_eps_reported, len_eps_surprise])\n",
    "        print('min', min_entries) \n",
    "        \n",
    "        print(len(usernames), len(quarter_dates), len(avg_checkins), len(avg_likes), len(avg_tac),\n",
    "             len(profit_over_revenue), len(expenses_over_revenue), len(eps_reported), len(eps_surprise))\n",
    "        print(len(revenue), len(profit), len(expenses))\n",
    "        \n",
    "        if expenses_over_revenue and profit_over_revenue:\n",
    "            while len(usernames) > min_entries:\n",
    "                usernames.pop()\n",
    "            while len(quarter_dates) > min_entries:\n",
    "                quarter_dates.pop()\n",
    "            while len(avg_checkins) > min_entries:\n",
    "                avg_checkins.pop()\n",
    "            while len(avg_likes) > min_entries:\n",
    "                avg_likes.pop()\n",
    "            while len(avg_tac) > min_entries:\n",
    "                avg_tac.pop()\n",
    "            while len(profit_over_revenue) > min_entries:\n",
    "                profit_over_revenue.pop()\n",
    "            while len(expenses_over_revenue) > min_entries:\n",
    "                expenses_over_revenue.pop()\n",
    "            while len(eps_reported) > min_entries:\n",
    "                eps_reported.pop()\n",
    "            while len(eps_surprise) > min_entries:\n",
    "                eps_surprise.pop()\n",
    "\n",
    "            if counter == 2:\n",
    "                new_df = pd.DataFrame({'Company': usernames, 'Quarter Dates': quarter_dates,\n",
    "                                        'Avg Checkins': avg_checkins, 'Avg Likes': avg_likes,\n",
    "                                        'Avg TAC': avg_tac, 'Profit/Revenue': profit_over_revenue, \n",
    "                                  'Expenses/Revenue': expenses_over_revenue, 'EPS': eps_reported, 'Surprise': eps_surprise})\n",
    "            else:\n",
    "                temp_df = pd.DataFrame({'Company': usernames, 'Quarter Dates': quarter_dates,\n",
    "                                        'Avg Checkins': avg_checkins, 'Avg Likes': avg_likes,\n",
    "                                        'Avg TAC': avg_tac, 'Profit/Revenue': profit_over_revenue, \n",
    "                                  'Expenses/Revenue': expenses_over_revenue, 'EPS': eps_reported, 'Surprise': eps_surprise})\n",
    "\n",
    "                new_df = pd.concat([new_df, temp_df], ignore_index=True)\n",
    "\n",
    "        #break\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = generate_master_dataframe(quarter_release_date, company_quarter_range, EPS_data, dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_pickle(\"./df_final.pkl\")\n",
    "#df_final.read_pickle(\"./df_final.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df_final.Surprise\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_final.drop(columns=['Surprise']), y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ct = ColumnTransformer([(\"company\", 'drop', ['Company']),\n",
    "                            (\"quarter dates\",'drop', ['Quarter Dates']),\n",
    "                            (\"checkins\", StandardScaler(), ['Avg Checkins']),\n",
    "                            (\"likes\", StandardScaler(), ['Avg Likes']),\n",
    "                            (\"tac\", StandardScaler(), ['Avg TAC']),\n",
    "                            (\"profit/revenue\", 'passthrough', ['Profit/Revenue']),\n",
    "                            (\"expenses/revenue\", 'passthrough', ['Expenses/Revenue']),\n",
    "                            (\"EPS\", 'passthrough', ['EPS']),\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "pipe = Pipeline(steps = ([('ColumnTransformer', ct),\n",
    "                           ('RandomForest', RandomForestRegressor())]))\n",
    "                          #('Ridge', Ridge())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid={'RandomForest__n_estimators': np.arange(1000, 1200, 10), 'RandomForest__max_depth': [18]},\n",
    "                 cv=5, verbose=1, n_jobs=2)\n",
    "\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(gs, open('finalized_model.sav', 'wb'))\n",
    "\n",
    "#gs = pickle.load(open('finalized_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_importance(gsobject):\n",
    "    importance = gs.best_estimator_.steps[1][1].feature_importances_\n",
    "    dataframe = pd.DataFrame(np.round(importance, decimals=3), columns=['Feature Importance'],\n",
    "                 index=['Avg Checkins','Avg Likes','Avg TAC','Profit/Revenue','Expenses/Revenue','EPS'])\n",
    "    return dataframe\n",
    "\n",
    "feature_importance = get_importance(gs)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.palettes import RdYlGn\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.transform import cumsum\n",
    "from bokeh.models.annotations import Title\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "x = feature_importance.to_dict()['Feature Importance']\n",
    "\n",
    "data = pd.Series(x).reset_index(name='value').rename(columns={'index':'Importance'})\n",
    "data['angle'] = data['value']/data['value'].sum() * 2*pi\n",
    "data['color'] = RdYlGn[len(x)]\n",
    "\n",
    "p = figure(plot_height=350, title=\"Pie Chart\", toolbar_location=None,\n",
    "           tools=\"hover\", tooltips=\"@Importance: @value\", x_range=(-0.5, 1.0))\n",
    "#p.name = 'Feature Importance'\n",
    "p.wedge(x=0, y=1, radius=0.4,\n",
    "        start_angle=cumsum('angle', include_zero=True), end_angle=cumsum('angle'),\n",
    "        line_color=\"white\", fill_color='color', legend='Importance', source=data)\n",
    "\n",
    "p.axis.axis_label=None\n",
    "p.axis.visible=False\n",
    "p.grid.grid_line_color = None\n",
    "\n",
    "t = Title()\n",
    "t.text = 'Ensemble Model Feature Importance'\n",
    "p.title = t\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prediction_to_test_set(test_set, prediction):\n",
    "    test_set['Predicted Surprise EPS'] = prediction\n",
    "    return test_set.loc[:, ['Company', 'Quarter Dates', 'Predicted Surprise EPS']].copy()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_predicted = add_prediction_to_test_set(X_test, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_predicted.to_pickle('X_test_predicted.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import timedelta\n",
    "def get_dow_price(date1):\n",
    "    \"\"\"Gets the close price for a Dow Jones index tracking fund for a given date\"\"\"\n",
    "    \n",
    "    def make_page_request(ticker, date1):\n",
    "        time.sleep(0.51)\n",
    "        datestr = date1.strftime('%Y-%m-%d')\n",
    "        dow_ticker = 'DIA'\n",
    "        nasdaq_ticker = 'QQQ'\n",
    "        url = 'https://api-v2.intrinio.com/securities/{}/prices'.format(ticker)\n",
    "        params={'api_key':'Ojg5ZDFmOGNmMzJiOWZjM2RjZGNhNDRiM2JiNWJkM2M0',\n",
    "           'start_date': datestr,\n",
    "           'end_date': datestr,\n",
    "           'frequency': 'daily'}\n",
    "        page = requests.get(url, params=params)\n",
    "        return page\n",
    "    \n",
    "    while True:\n",
    "        page = make_page_request('DIA', date1)\n",
    "        dowjones = json.loads(page.text)\n",
    "        if dowjones['stock_prices']:\n",
    "            dj_close = dowjones['stock_prices'][0]['close']\n",
    "            break\n",
    "        else:\n",
    "            date1 += timedelta(days=1)\n",
    "    \n",
    "    return dj_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dow_growth():\n",
    "    \n",
    "    quarters = [(datetime(2017, 1, 1).date(), datetime(2017, 3, 31).date()),\n",
    "               (datetime(2017, 4, 1).date(), datetime(2017, 6, 30).date()),\n",
    "               (datetime(2017, 7, 1).date(), datetime(2017, 9, 30).date()),\n",
    "               (datetime(2017, 10, 1).date(), datetime(2017, 12, 31).date()),\n",
    "               (datetime(2018, 1, 1).date(), datetime(2018, 3, 31).date()),\n",
    "               (datetime(2018, 4, 1).date(), datetime(2018, 6, 30).date()),\n",
    "               (datetime(2018, 7, 1).date(), datetime(2018, 9, 30).date()),\n",
    "               (datetime(2018, 10, 1).date(), datetime(2018, 12, 31).date())]\n",
    "\n",
    "    dow_profit = []\n",
    "    nasdaq_profit = []\n",
    "    \n",
    "    for date_range in quarters:\n",
    "        dow1 = get_dow_price(date_range[1])\n",
    "        dow0 = get_dow_price(date_range[0]) \n",
    "        dow_profit.append(dow1 - dow0)\n",
    "        \n",
    "        \n",
    "    return dow_profit\n",
    "        \n",
    "        \n",
    "dow_profit= get_dow_growth()        \n",
    "dow_profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dow_quarter_prices():\n",
    "    \n",
    "    def make_page_request(ticker, start, end):\n",
    "        time.sleep(0.51)\n",
    "        startstr = start.strftime('%Y-%m-%d')\n",
    "        endstr = end.strftime('%Y-%m-%d')\n",
    "        dow_ticker = 'DIA'\n",
    "        nasdaq_ticker = 'QQQ'\n",
    "        url = 'https://api-v2.intrinio.com/securities/{}/prices'.format(ticker)\n",
    "        params={'api_key':'Ojg5ZDFmOGNmMzJiOWZjM2RjZGNhNDRiM2JiNWJkM2M0',\n",
    "           'start_date': startstr,\n",
    "           'end_date': endstr,\n",
    "           'frequency': 'daily'}\n",
    "        page = requests.get(url, params=params)\n",
    "        return page\n",
    "    \n",
    "    master_date_price_dict = {}\n",
    "    \n",
    "    for year in [2017, 2018]:\n",
    "        \n",
    "        for quarter in ['Jan 1 - Mar 31', 'Apr 1 - Jun 30', 'Jul 1 - Sep 30', 'Oct 1 - Dec 31']:\n",
    "    \n",
    "            if quarter == 'Jan 1 - Mar 31':\n",
    "                start = datetime(year, 1, 1).date()\n",
    "                end = datetime(year, 3, 31).date()\n",
    "            elif quarter == 'Apr 1 - Jun 30':\n",
    "                start = datetime(year, 4, 1).date()\n",
    "                end = datetime(year, 6, 30).date()\n",
    "            elif quarter == 'Jul 1 - Sep 30':\n",
    "                start = datetime(year, 7, 1).date()\n",
    "                end = datetime(year, 9, 30).date()\n",
    "            elif quarter == 'Oct 1 - Dec 31':\n",
    "                start = datetime(year, 10, 1).date()\n",
    "                end = datetime(year, 12, 31).date()\n",
    "\n",
    "            date_price_dict = {}\n",
    "\n",
    "            page = make_page_request('DIA', start, end)\n",
    "            dowjones = json.loads(page.text)\n",
    "            if dowjones['stock_prices']:\n",
    "                for entry in dowjones['stock_prices']:\n",
    "                    date_price_dict[entry['date']] = entry['close']\n",
    "\n",
    "            master_date_price_dict[(str(year), quarter)] = date_price_dict\n",
    "    \n",
    "    return master_date_price_dict\n",
    "\n",
    "dowjones_date_quarter_price_dict = get_dow_quarter_prices()\n",
    "dowjones_date_quarter_price_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dowjones_date_quarter_price_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(dowjones_date_quarter_price_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('dowjones_date_quarter_price_dict.pickle', 'rb') as handle:\n",
    "#     dowjones_date_quarter_price_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "def get_profit_per_release(dataframe):\n",
    "    \n",
    "    profit = []\n",
    "    cost = []\n",
    "    running_investment = 0\n",
    "    for index in range(len(dataframe)):       \n",
    "        row = dataframe.iloc[index]\n",
    "        company = row['Company']\n",
    "        report_date = row['Report Date']\n",
    "        eps = row['Predicted Surprise EPS']\n",
    "        \n",
    "        ticker = get_stock_info(company)['ticker']\n",
    "        \n",
    "        result_dict = get_stock_price_from_ticker(ticker, report_date - timedelta(days=20),\n",
    "                                                  report_date + timedelta(days=20))\n",
    "        date= []\n",
    "        price = []\n",
    "        for entry in result_dict:\n",
    "            date.append(datetime.strptime(entry['date'], '%Y-%m-%d').date())\n",
    "            price.append(entry['close'])\n",
    "            \n",
    "        start = report_date - timedelta(days=1)\n",
    "        end = report_date\n",
    "\n",
    "        start_price = None\n",
    "        end_price = None\n",
    "\n",
    "        while True:\n",
    "            for d, p in zip(date, price):\n",
    "                if d == start:\n",
    "                    start_price = p\n",
    "                elif d == end:\n",
    "                    end_price = p            \n",
    "            if start_price is None:\n",
    "                start -= timedelta(days=1)\n",
    "            if end_price is None:\n",
    "                end += timedelta(days=1)\n",
    "            if start_price is not None and end_price is not None:\n",
    "                break\n",
    "                \n",
    "        if eps > 0:\n",
    "            profit.append(end_price - start_price)\n",
    "            cost.append(start_price)\n",
    "        else:\n",
    "            profit.append(0)\n",
    "            cost.append(0)\n",
    "        \n",
    "    return (cost, profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('select_cost.pickle', 'wb') as handle:\n",
    "#     pickle.dump(select_cost, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('select_cost.pickle', 'rb') as handle:\n",
    "#     select_cost = pickle.load(handle)\n",
    "\n",
    "# with open('select_profit.pickle', 'wb') as handle:\n",
    "#     pickle.dump(select_profit, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('select_profit.pickle', 'rb') as handle:\n",
    "#     select_profit = pickle.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dev_new",
   "language": "python",
   "name": "capstone-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
